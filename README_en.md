# LLM Evaluator

[中文版本](README.md)

> This README was generated by Qwen Code

A comprehensive framework for evaluating Large Language Models (LLMs) with support for safety, capability, and refusal assessment. This framework enables unified evaluation of model security attacks and defenses.

## Features

- Safety evaluation with various attack methods
- Capability assessment across multiple benchmarks
- Support for API-based and local model inference
- Configurable evaluation pipelines
- Cache management for efficient repeated evaluations
- Metrics computation for performance analysis

## Environment Setup

### Prerequisites

- Python 3.12 or higher
- [uv](https://github.com/astral-sh/uv) for dependency management
- Redis server (for caching, optional)
- GPU resources (for local model inference, optional)

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yibotongxue/llm-evaluator
   cd llm-evaluator
   ```

2. Install dependencies using uv:
   ```bash
   uv sync
   ```

3. Set up environment variables:
   Export the Qwen API key (required for API-based inference at this stage):
   ```bash
   export QWEN_API_KEY=your_qwen_api_key_here
   ```

## Running the Evaluator

### Basic Usage

To run the safety evaluation with a specific configuration:

```bash
QWEN_API_KEY=your_qwen_api_key_here uv run -m llm_evaluator.benchmark.benchmark --config-file-path ./configs/safety.yaml
```

### Configuration

The framework uses YAML configuration files to define:
- Model settings (local or API-based)
- Evaluation benchmarks
- Attack methods
- Metrics computation
- Cache settings

Example configuration files can be found in the `configs/` directory.

### Available Benchmarks

The framework supports several benchmarks including:
- AdvBench
- StrongReject
- JBB-Behaviors
- WildJailbreak (Vanilla and Adversarial)

### Attack Methods

Various prompt-based attack methods are implemented:
- Base64Attack
- AIMAttack
- Dev Mode v2
- BetterDAN
- And many others

## Customization

To customize the evaluation:

1. Modify or create new configuration files in the `configs/` directory
2. Add new attack methods in the `llm_evaluator/prompts/attack/` directory
3. Implement new metrics in the `llm_evaluator/metrics/` directory
4. Add new data formats in the `llm_evaluator/data/` directory

## Output

Evaluation results are saved in the output directory specified in the configuration file, typically including:
- Full results with detailed metrics
- Brief summary results
- Configuration backup
